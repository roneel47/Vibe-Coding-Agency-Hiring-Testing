{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A:\n",
    "\n",
    "### 1.1 Discrete Steps Breakdown\n",
    "\n",
    "The end-to-end lifecycle decomposes into 7 sequential/parallel stages:\n",
    "\n",
    "| Step | Input | Output | Success Criteria | Failure Handling |\n",
    "|------|-------|--------|------------------|------------------|\n",
    "| **1. PR Ingestion** | PR metadata, diff, repo config | Normalized PR context (files changed, lines added/removed, commit msg, author, linked issues) | PR parsed successfully, all metadata extracted | Retry ingestion; alert on malformed PR |\n",
    "| **2a. Static/SCA Analysis** | Code diff, dependencies, config files | SAST/SCA report (vulns, CWE refs, severity, LOC) | No critical vulns or flagged for review | Block merge; escalate critical findings |\n",
    "| **2b. AI Code Review** | Diff, repo patterns, language rules, service tier | Review report (issues: style/perf/logic, severity, line refs, fix hints) | <5 style issues per 100 LOC, 0 critical logic bugs | Human review triggered; suggestions logged |\n",
    "| **3. Human Gate** | AI review output, SCA report, business context | Approval/rejection decision, optional comments | Decision made within 2 hours of PR submission | Escalation to oncall; manager override option |\n",
    "| **4. CI Tests & QA** | Approved PR, test suite, deployment artifacts | Test results (pass/fail, coverage %), build artifacts | 90%+ coverage, all tests pass, build succeeds | Automated rollback to previous version; notify team |\n",
    "| **5. Canary/Blue-Green Deploy** | Build artifacts, target env config, feature flags | Deployment status, initial metrics (errors, latency, traffic split) | 0% error rate in canary (1% traffic), <5% latency increase | Auto-rollback triggered; incident alert |\n",
    "| **6. Metrics Guardrails & Auto-Rollback** | Prod metrics (errors, latency, resource usage), SLOs | Decision: continue / rollback, alert/incident record | SLOs maintained for 15 min; no P1 incidents | Automatic rollback; post-incident review queued |\n",
    "| **7. Post-Deploy Feedback Loop** | Deploy outcome, metrics, developer feedback, production incidents | Signals for model training: FP/FN rates, deploy success pattern, latency correlation | Feedback ingested; trends identified for model refinement | Manual review; no model corruption |\n",
    "\n",
    "### 1.2 Parallelism & Blocking\n",
    "\n",
    "**Parallel (non-blocking):**\n",
    "- Step 2a (Static/SCA) and 2b (AI Code Review) run **concurrently** after PR ingestion.\n",
    "- Both complete before Step 3 (human gate).\n",
    "\n",
    "**Blocking (sequential):**\n",
    "- Step 1 (ingestion) → Step 2a+2b (analysis) → Step 3 (human gate) → Step 4 (CI) → Step 5 (deploy) → Step 6 (guardrails) → Step 7 (feedback).\n",
    "\n",
    "**Critical Decision Points:**\n",
    "1. **Human Gate (Step 3):** Humans review AI + SCA findings; approve/reject.\n",
    "2. **Auto-Rollback Trigger (Step 6):** Metrics breach SLOs → automatic rollback without manual intervention.\n",
    "3. **Feedback Signal Collection (Step 7):** False positives, deploy outcomes, production incidents inform model retraining.\n",
    "\n",
    "### 1.3 Key Handoff Points & Context Passing\n",
    "\n",
    "| Handoff | From | To | Data Passed |\n",
    "|---------|------|----|----|\n",
    "| Post-ingestion | PR Ingestion | Static/SCA + AI Review | Normalized diff, repo patterns, service risk tier, coding standards |\n",
    "| Analysis → Human | Static/SCA + AI Review | Human Gate | Aggregated findings (severity, locations, fix hints), coverage metrics, potential risk scores |\n",
    "| Approval → CI | Human Gate | CI/Tests | Decision token, approved commit SHA, deployment constraints |\n",
    "| Build → Deploy | CI/Tests | Canary Deploy | Build artifacts (Docker image, checksum), test coverage %, passed checks |\n",
    "| Deploy → Guardrails | Canary Deploy | Metrics Guardrails | Deployment ID, feature flag state, baseline metrics, traffic allocation |\n",
    "| Production → Feedback | Metrics Guardrails + Incidents | Feedback Loop | Deploy status (success/rollback), post-deploy metrics, user impact, incident correlation |\n",
    "| Feedback → Model | Feedback Loop | AI Review (next cycle) | FP/FN counts, deploy success patterns, code patterns associated with failures |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "The system achieves <4-hour review SLA through parallel analysis, human gate on high-risk changes, automated testing/deployment with metrics-driven rollback, and continuous feedback loops for model improvement. \n",
    "\n",
    "![Part A Swimlane Diagram](./images/part-a-swimlane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:\n",
    "\n",
    "### 2.1 AI Prompting Strategy: Two Major Steps\n",
    "\n",
    "**Step 1 → Step 2: AI Code Review → Risk Triage**\n",
    "\n",
    "#### Prompt 1: AI Code Reviewer\n",
    "\n",
    "**System Role/Persona:**\n",
    "```\n",
    "You are an expert code reviewer. Your role is to identify code quality, \n",
    "performance, security, and maintainability issues in pull requests. \n",
    "You prioritize by impact and actionability. You maintain consistency with \n",
    "the codebase's patterns and the team's coding standards.\n",
    "```\n",
    "\n",
    "**Structured Input Format:**\n",
    "```json\n",
    "{\n",
    "  \"diff\": \"<unified diff of changes>\",\n",
    "  \"file_language\": \"python|javascript|go|...\",\n",
    "  \"service_name\": \"order-service\",\n",
    "  \"service_tier\": \"critical|standard|background\",\n",
    "  \"coding_standards_url\": \"https://wiki.internal/python-standards\",\n",
    "  \"recent_incidents\": [\"CWE-89 SQL injection in payment module (Dec 2025)\"],\n",
    "  \"dependencies_changed\": [\"sqlalchemy==2.0.1\"],\n",
    "  \"test_coverage_before\": 82,\n",
    "  \"test_coverage_after\": 84\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"issues\": [\n",
    "    {\n",
    "      \"id\": \"REVIEW-001\",\n",
    "      \"type\": \"security|performance|style|logic\",\n",
    "      \"severity\": \"critical|high|medium|low\",\n",
    "      \"line_numbers\": [42, 45],\n",
    "      \"title\": \"Unparameterized SQL query\",\n",
    "      \"description\": \"Line 42 concatenates user input into SQL without parameterization\",\n",
    "      \"cwe\": \"CWE-89\",\n",
    "      \"fix_hint\": \"Use SQLAlchemy ORM or parameterized queries\",\n",
    "      \"code_snippet\": \"SELECT * FROM users WHERE id = \" + user_id\"\n",
    "    }\n",
    "  ],\n",
    "  \"summary\": {\n",
    "    \"total_issues\": 3,\n",
    "    \"critical\": 0,\n",
    "    \"high\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"risk_score\": 35  // 0-100\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Examples of Good vs Bad Responses:**\n",
    "\n",
    "*Good:*\n",
    "- Identifies CWE-89 in parameterized query construction.\n",
    "- Flags performance issue: O(n²) loop with DB query per iteration; suggests batch operation.\n",
    "- Suggests applying existing logging pattern in codebase.\n",
    "\n",
    "*Bad:*\n",
    "- Vague complaint like \"this code is not clean.\"\n",
    "- False positive: flagging a safe helper function as security risk.\n",
    "- Ignoring team's established patterns (e.g., reviewing async/await code as synchronous).\n",
    "\n",
    "**Error Handling:**\n",
    "```\n",
    "- If diff is malformed: Return {\"error\": \"Unparseable diff\", \"status\": \"failed\"}\n",
    "- If language is unsupported: Return {\"error\": \"Language not supported\", \"supported\": [list]}\n",
    "- If no issues found: Return {\"issues\": [], \"summary\": {\"total_issues\": 0}}\n",
    "- If analysis times out (>30s): Escalate to human reviewer; mark as \"timeout\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Prompt 2: Risk Triage & Sign-Off\n",
    "\n",
    "**System Role/Persona:**\n",
    "```\n",
    "You are a deployment risk assessor. Given code review findings and test coverage, \n",
    "you determine deployment readiness: Green (safe), Yellow (review advised), \n",
    "Red (block until resolved). You balance agility with safety and consider \n",
    "the service's business criticality.\n",
    "```\n",
    "\n",
    "**Structured Input Format:**\n",
    "```json\n",
    "{\n",
    "  \"service_name\": \"order-service\",\n",
    "  \"service_tier\": \"critical\",\n",
    "  \"pr_size\": \"medium\",  // small | medium | large\n",
    "  \"review_issues\": [\n",
    "    {\"severity\": \"critical\", \"count\": 0},\n",
    "    {\"severity\": \"high\", \"count\": 1},\n",
    "    {\"severity\": \"medium\", \"count\": 2}\n",
    "  ],\n",
    "  \"test_coverage_delta\": 2,  // percentage points\n",
    "  \"deployment_frequency\": \"weekly\",\n",
    "  \"recent_rollbacks\": 1,  // past 30 days\n",
    "  \"slo_breach_last_week\": false\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"recommendation\": \"GREEN|YELLOW|RED\",\n",
    "  \"reasoning\": \"Service is critical tier; high-severity security issue identified. Recommend human review before merge.\",\n",
    "  \"required_actions\": [\n",
    "    \"Resolve CWE-89 SQL injection\",\n",
    "    \"Increase test coverage by 2%\"\n",
    "  ],\n",
    "  \"human_review_requested\": true,\n",
    "  \"deployment_constraints\": {\n",
    "    \"require_canary\": true,\n",
    "    \"traffic_split_percent\": 5,\n",
    "    \"rollback_threshold_error_rate\": 0.02\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Error Handling:**\n",
    "- If required fields missing: Return error with list of missing fields.\n",
    "- If triage confidence low: Flag for human review; return {\"recommendation\": \"YELLOW\", \"confidence\": 0.65}\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Handling Challenging Scenarios\n",
    "\n",
    "| Scenario | AI Prompt Adaptation | Example |\n",
    "|----------|----------------------|---------|\n",
    "| **Obscure libraries** | Provide library docs in context; ask AI to flag if unfamiliar. If unrecognized, escalate. | LLM detects `sklearn.ensemble.IsolationForest` used; adds note \"Review domain context\"; human verifies algorithm choice |\n",
    "| **Security reviews** | Include CWE/OWASP context; require evidence (line numbers, exploit path). Use strict rules for injection, auth, crypto. | Hardcoded API key → flagged as CWE-798; requires immediate removal or secret manager integration |\n",
    "| **Database perf** | Provide schema, query count baseline, SLO (e.g., p99 <100ms). Flag N+1 patterns, missing indexes. | AI detects loop with DB query; suggests: batch fetch, join, or caching with TTL |\n",
    "| **Legacy code** | Provide legacy patterns & deprecation roadmap; focus on blocking bugs, not style. | Don't flag older async pattern if team is mid-migration; flag actual bugs instead |\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Ensuring Prompt Effectiveness & Consistency\n",
    "\n",
    "1. **Rubric-Based Scoring:** Define scoring rubric (e.g., detection of CWE-89, false positive rate <10%, actionable suggestions >80%). Test prompts against known vulnerabilities.\n",
    "\n",
    "2. **Regression Test Suite:** Maintain set of 20+ known issues (past security bugs, performance problems, style violations) and verify AI catches them.\n",
    "\n",
    "3. **Style Guide & Examples:** Embed team's coding standards and real examples of good/bad findings in prompt context.\n",
    "\n",
    "4. **A/B Testing:** Run same PR through two prompt versions; measure time-to-fix, developer satisfaction, false positive rate.\n",
    "\n",
    "5. **Feedback Loop:** Collect developer feedback on review quality; adjust prompt seasonally.\n",
    "\n",
    "6. **False Positive Tracking:** Log all flagged issues; if 3+ developers dismiss a finding type, retune prompt logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:\n",
    "\n",
    "### 3.1 System Architecture for Reusability Across Teams/Projects\n",
    "\n",
    "The system achieves reusability through a **modular, policy-driven, multi-adapter architecture**:\n",
    "\n",
    "#### Core Design Principles\n",
    "\n",
    "1. **Adapter Pattern for VCS/CI/Cloud Targets**\n",
    "   - Abstract away GitHub/GitLab/Bitbucket differences with a unified PR interface\n",
    "   - Support Jenkins, GitHub Actions, GitLab CI, Azure Pipelines through pluggable orchestrators\n",
    "   - Cloud-agnostic deployment adapters (AWS CloudFormation, Azure ARM, Terraform)\n",
    "   - Example: Teams on GitLab + on-prem Kubernetes use same review engine; adapter translates API calls\n",
    "\n",
    "2. **Configuration-Driven Customization**\n",
    "   - No code changes per team; all variance in YAML/JSON configs stored per service/team\n",
    "   ```yaml\n",
    "   # service-config.yaml\n",
    "   service_name: order-service\n",
    "   language: python\n",
    "   framework: fastapi\n",
    "   deployment_target: aws-eks\n",
    "   deployment_strategy: canary  # or blue-green, rolling\n",
    "   review_rules: \"standard-python-2025\"\n",
    "   compliance_profile: \"pci-dss\"\n",
    "   sla_review_hours: 4\n",
    "   sla_deploy_minutes: 30\n",
    "   rollback_threshold_error_rate: 0.02\n",
    "   rollback_threshold_latency_p99_ms: 500\n",
    "   owners: [\"platform-team\", \"order-squad\"]\n",
    "   ```\n",
    "\n",
    "3. **Language/Framework Abstraction**\n",
    "   - AI reviewer trained on multiple languages; use language-specific linters as fallback\n",
    "   - Maintain language packs: Python (FastAPI, Django, async patterns), Go (Gin, gRPC), Node (Express, NestJS), Java (Spring Boot)\n",
    "   - Each pack includes: common antipatterns, library-specific rules, security checklist, performance baselines\n",
    "   - Example: Node pack knows to check for Promise leak patterns; Python pack checks async/await gotchas\n",
    "\n",
    "4. **Team-Specific Coding Standards**\n",
    "   - Store per-team style guides in a standards registry (versioned, auditable)\n",
    "   - AI reviewer includes team's guide in context; enforces without hardcoding\n",
    "   - Enable teams to override global rules for their domain (e.g., fintech = stricter crypto checks; social = aggressive caching OK)\n",
    "   ```\n",
    "   standards/\n",
    "     global/\n",
    "       - security-baseline.yaml\n",
    "       - performance-baseline.yaml\n",
    "     team/\n",
    "       - order-squad-standards.yaml  (overrides global for fintech rules)\n",
    "       - notifications-squad-standards.yaml  (higher async/cache tolerance)\n",
    "   ```\n",
    "\n",
    "5. **Deployment Target Flexibility**\n",
    "   - Deploy adapter layer translates service config → target-specific manifest\n",
    "   - Canary/blue-green strategy configurable per service, environment, or risk tier\n",
    "   - Metrics guardrails adapted per cloud provider's native observability (AWS CloudWatch, Azure Monitor, Datadog, Prometheus)\n",
    "   ```\n",
    "   deployment/\n",
    "     adapters/\n",
    "       - aws-eks-adapter\n",
    "       - azure-aks-adapter\n",
    "       - gcp-gke-adapter\n",
    "       - on-prem-k8s-adapter\n",
    "       - lambda-adapter (for serverless)\n",
    "   ```\n",
    "\n",
    "6. **Compliance & Industry-Specific Rules**\n",
    "   - Compliance profiles: PCI-DSS, HIPAA, SOC2, GDPR\n",
    "   - Auto-enable SAST/DAST rules, audit logging, encryption checks per profile\n",
    "   - Example: Healthcare team gets HIPAA profile → AI review flags unencrypted PII, requires audit trails, blocks deploy without encryption\n",
    "   ```\n",
    "   compliance/\n",
    "     - pci-dss-profile.yaml (strict DB access, no plaintext secrets)\n",
    "     - hipaa-profile.yaml (PII encryption, audit logging, retention)\n",
    "     - soc2-profile.yaml (change management, incident correlation)\n",
    "   ```\n",
    "\n",
    "#### Architecture Diagram Reference\n",
    "The swimlane diagram shows core services that support reusability:\n",
    "- **Ingestion Adapters** (left side): Normalize PR events from any VCS\n",
    "- **Policy/Config Service** (center): Loads service/team configs, applies rules\n",
    "- **Model Router** (center): Routes to language-specific AI models or falls back to linters\n",
    "- **Static/SCA Scanners** (right side): Pluggable SAST/DAST/SCA tools (SonarQube, Snyk, Veracode)\n",
    "- **CI/CD Orchestrator** (right side): Dispatches to Jenkins, Actions, GitLab CI, etc.\n",
    "- **Artifact Store** (bottom): Centralized storage for review reports, metrics, feedback\n",
    "- **Observability Hooks** (bottom): Stream metrics to any monitoring tool\n",
    "\n",
    "![Part C Architecture Diagram](./images/part-c-architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Continuous Improvement Over Time\n",
    "\n",
    "The system learns and improves through **structured feedback loops** that track signal sources:\n",
    "\n",
    "#### Feedback Loop Architecture\n",
    "\n",
    "| Signal Source | Collection Method | Use Case | Improvement |\n",
    "|---|---|---|---|\n",
    "| **False Positives (FP)** | Developers dismiss AI findings or override approvals | \"This is not a real issue\" | Retune prompt; lower severity threshold for specific issue type |\n",
    "| **False Negatives (FN)** | Production incidents traced to missed code review findings | \"AI missed a critical bug\" | Augment training data; add regression test to review suite |\n",
    "| **Deploy Success/Failure** | Compare deploy outcomes (pass/rollback/incident) to code patterns | \"This pattern = risky\" | Weight high-risk patterns in AI triage; trigger manual review for similar PRs |\n",
    "| **Developer Feedback** | In-tool survey: \"Was this review helpful?\" 1–5 scale | \"Feedback too noisy\" or \"Feedback too lenient\" | Adjust prompt specificity; retrain rubric |\n",
    "| **Metrics Correlation** | Post-deploy monitoring (error rate, latency, resource usage) vs. code changes | \"This type of change causes latency spikes\" | Flag patterns in future reviews; add performance baseline check |\n",
    "| **Production Incidents** | Correlate P1/P2 incidents back to recent deployments → code review | \"Bug slipped through code review\" | Root-cause analysis; update standards; mark codebase area as high-risk |\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "1. **Telemetry Collection**\n",
    "   ```\n",
    "   Every review finding stores:\n",
    "   - Finding ID, severity, type, line refs\n",
    "   - Developer action: approved/overridden/fixed\n",
    "   - Outcome: did bug reach prod? incident correlation?\n",
    "   - Feedback: developer rating (1-5), comment\n",
    "   - Time to deploy, deploy status, post-deploy metrics\n",
    "   ```\n",
    "\n",
    "2. **Feedback Pipeline**\n",
    "   ```\n",
    "   workflow:\n",
    "     1. Collect daily: FP counts, FN counts, deploy outcomes, incident correlations\n",
    "     2. Aggregate weekly: FP/FN rates by issue type, deploy success rate by service tier\n",
    "     3. Analyze monthly: Patterns in high-FP types, correlation of code patterns to prod incidents\n",
    "     4. Retrain quarterly: Update AI prompts, add regression tests, adjust severity thresholds\n",
    "   ```\n",
    "\n",
    "3. **Learning Loop Examples**\n",
    "   - **FP Reduction:** \"CWE-89 false positives dropped 40% after adding SQLAlchemy ORM context to prompt.\"\n",
    "   - **FN Reduction:** \"Added async/await deadlock pattern to regression suite after Q1 incident; now caught in 95% of reviews.\"\n",
    "   - **Risk Weighting:** \"Services in fintech tier deploy 3x more frequently than social tier; weight security issues heavier for fintech.\"\n",
    "   - **Developer Adoption:** \"After shortening review summaries and adding fix hints, developer satisfaction rose from 3.1 to 4.3/5.\"\n",
    "\n",
    "4. **Governance & Rollback**\n",
    "   - All prompt updates, threshold changes, compliance rule updates tracked in version control\n",
    "   - A/B test new prompt versions on 10% of PRs before rollout\n",
    "   - If FP rate spikes >20%, auto-rollback to previous prompt version; alert on-call\n",
    "\n",
    "5. **External Signals**\n",
    "   - Subscribe to security advisories (CVE feeds) and auto-flag affected dependencies\n",
    "   - Track industry incidents (e.g., Log4j vulnerability) and retrain rules to catch similar patterns\n",
    "   - Quarterly review of team feedback + incident data with platform & squad leads to refine priorities\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The system is **infinitely reusable** through configuration, adapters, and language packs—no code changes needed per team/project. It **continuously improves** by ingesting signals from false positives/negatives, deploy outcomes, incidents, and developer feedback into a monthly retraining cycle that sharpens prompts, raises detection rates, and reduces friction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:\n",
    "\n",
    "### 4.1 Implementation Prioritization & 6-Month Roadmap\n",
    "\n",
    "#### MVP Definition (Months 1–2)\n",
    "\n",
    "The minimum viable system focuses on **speed and developer trust**, not breadth:\n",
    "\n",
    "**In Scope:**\n",
    "- **1 Programming Language:** Python (covers ~40% of codebase; fastest to validate)\n",
    "- **1 VCS:** GitHub (market standard; strongest API)\n",
    "- **1 CI/CD:** GitHub Actions (native integration; low ops cost)\n",
    "- **Core Workflow:** PR → Static/SCA scan → AI review → Human gate → CI tests → Canary deploy → Metrics guardrails → Auto-rollback\n",
    "- **Success Metric:** <4-hour review SLA on 2 pilot services\n",
    "\n",
    "**Out of Scope:**\n",
    "- Multi-language support (Phase 2)\n",
    "- GitLab, Bitbucket, Jenkins (Phase 2)\n",
    "- Compliance logging (Phase 3)\n",
    "- Advanced ML-driven failure prediction (Phase 4+)\n",
    "\n",
    "**MVP Deliverables:**\n",
    "- GitHub App (listens to PR events; triggers review workflow)\n",
    "- AI reviewer service (calls LLM with code diff + context)\n",
    "- Policy/config service (loads review rules per service)\n",
    "- Canary deploy adapter (AWS ECS/EKS)\n",
    "- Metrics dashboard (basic: error rate, latency, traffic split)\n",
    "\n",
    "---\n",
    "\n",
    "![Part D 6-Month Roadmap](./images/part-d-roadmap.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Phase Timeline & Success Metrics\n",
    "\n",
    "| Phase | Duration | Scope | Key Deliverables | Success Metrics |\n",
    "|-------|----------|-------|------------------|-----------------|\n",
    "| **1: MVP** | Weeks 1–8 | Python, GitHub, Actions, core review loop | GitHub App, AI reviewer, policy engine, canary adapter | <4h review SLA on 2 services; 90%+ SCA + AI coverage |\n",
    "| **2: Pilot** | Weeks 9–12 | Deploy to 2 real services; add SCA + automated rollback | SCA integration (Snyk/SonarQube), blue-green deploy, feedback collection | <10% failed canaries; 95%+ on-time rollbacks; 3.5/5 dev satisfaction |\n",
    "| **3a: Staged Rollout (Weeks 13–16)** | Weeks 13–16 | Add Java + Node.js; roll out to 10 services | Multi-language AI models, language packs, pre-built policy templates | 85%+ detection rate across 3 languages; <15% false positive rate |\n",
    "| **3b: Staged Rollout (Weeks 17–20)** | Weeks 17–20 | Add observability; roll out to 30+ services | Prometheus/Datadog integration, advanced guardrails (resource usage, throughput) | SLA maintained <2% breach rate; MTTR <30 min |\n",
    "| **4: Hardening** | Weeks 21–24 | Compliance, FP/FN tuning, team feedback loop | Audit logging, compliance profiles, FP/FN tracking dashboard, feedback portal | Audit-ready; <10% FP rate; >90% dev adoption; incident correlation <1h |\n",
    "\n",
    "---\n",
    "\n",
    "#### Pilot Program Strategy (Months 3)\n",
    "\n",
    "**Pilot Scope:**\n",
    "- **2 Services:** \n",
    "  - `order-service` (critical tier, Python, high review volume → tests value)\n",
    "  - `notification-service` (standard tier, Python, lower risk → builds confidence)\n",
    "- **Duration:** 4 weeks (Weeks 9–12)\n",
    "- **Participants:** Full squad (~8 devs per service); product/eng leads; on-call\n",
    "- **Opt-Out Policy:** Developers can revert to manual reviews if system breaks down; no penalties\n",
    "\n",
    "**Pilot Goals:**\n",
    "1. Achieve <4-hour review SLA 95% of the time\n",
    "2. Zero critical bugs slip past AI review into prod (FN rate <1%)\n",
    "3. Developer satisfaction ≥3.5/5 (feedback surveys)\n",
    "4. <10% canary failure rate (rollback decisions accurate)\n",
    "5. Collect 50+ real production incidents and code patterns for ML training\n",
    "\n",
    "**Pilot Failure Criteria** (auto-rollback to manual reviews):\n",
    "- Review SLA breached >20% of the time\n",
    "- >5% critical bugs reach production (traced to missed code review)\n",
    "- Developer satisfaction <3.0/5\n",
    "- System downtime >99% availability\n",
    "\n",
    "**Pilot Engagement:**\n",
    "- Daily standup with squads; weekly feedback sessions\n",
    "- Slack channel for issues; oncall engineer on rotation\n",
    "- Weekly metrics review (SLA, detection rate, FP count)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Risk Mitigation Strategy\n",
    "\n",
    "#### Key Risks & Mitigation Plans\n",
    "\n",
    "| Risk | Impact | Probability | Mitigation |\n",
    "|------|--------|-------------|-----------|\n",
    "| **AI makes incorrect review decisions** (FN: misses bug; FP: blocks valid PR) | Deploy breaks; devs lose trust; adoption fails | High | **Human-in-the-loop gate:** AI review tagged \"informational\" until <5% FP achieved. Manual approval required for critical services initially. Regression test suite (50+ known bugs) validates AI before rollout. |\n",
    "| **System downtime during critical deployments** | Blocked deployments; manual workaround costly; SLA miss | Medium | **Circuit breaker:** If review service unavailable >5 min, auto-escalate to human reviewer (no block). Blue-green deployment for core services; run review service in 2+ AZs. 99.5% SLA target. |\n",
    "| **Integration fragility** (GitHub API breaks; CI/CD timeout; metrics stream fails) | Partial workflow failures; inconsistent state | Medium | **Adapter pattern:** Decouple integrations via message queue. Retry logic with exponential backoff. Fallback: if integration fails, escalate to human; log for post-mortem. Monitor adapter health separately. |\n",
    "| **Developer resistance** (\"AI review is noisy\"; \"slow down my deploys\") | Low adoption; manual reviews persist; ROI ∅ | Medium | **Early engagement:** Involve 1–2 squad leads co-design prompts. Demo early wins (e.g., caught critical auth bug). Gather feedback weekly; adjust sensitivity per squad. Tie SLA improvements to performance evals. Celebrate wins. |\n",
    "| **Compliance & audit requirements** (PCI-DSS, HIPAA, SOC2 mandates) | Non-compliant deployments; audit failure; legal risk | Medium | **Compliance by design:** Embed audit logging from M1. Track all review decisions, overrides, rollbacks. Immutable audit trail in central store. Monthly compliance report. Engage security/legal early in design. |\n",
    "| **Model quality degrades over time** (data drift; adversarial code patterns) | FN/FP rates spike; detection rate ⬇️ | Medium | **Continuous monitoring:** Track FP/FN/deployment success weekly. A/B test prompt changes on 10% of PRs. Monthly retraining cycle. Auto-rollback if metrics degrade >10%. |\n",
    "| **Secrets/credentials leak through review reports** | Security incident; data breach | Low | **Secrets masking:** Redact AWS keys, API tokens, DB passwords before storing review reports. Scan artifacts for secrets using dedicated scanner (GitGuardian, TruffleHog). Encrypt artifact storage. |\n",
    "| **Latency: AI review takes too long** (>1h per PR → SLA miss) | Users bypass system; manual reviews resume; <4h SLA impossible | Medium | **Optimization:** Batch requests; cache model loading; use smaller model variants for standard reviews. SLA = <5 min avg review time. Monitor p95 latency. If >10 min, escalate to human async. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Risk Rollout Strategy\n",
    "\n",
    "**Phased Confidence Building:**\n",
    "- **Weeks 1–4:** AI review is advisory only (tag \"informational\"); no gates applied\n",
    "- **Weeks 5–8:** AI review can suggest gate, but human approval always required before merge\n",
    "- **Weeks 9–12:** Auto-gate for low-risk PRs (small <50 LOC, standard tier, green AI + SCA); human gate for high-risk\n",
    "- **Weeks 13+:** Full auto-gate once FN rate proven <1% and dev satisfaction ≥4/5\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Tool Selection & Integration Strategy\n",
    "\n",
    "#### Code Review & VCS\n",
    "\n",
    "| Tool | Choice | Rationale | Integration |\n",
    "|------|--------|-----------|-------------|\n",
    "| **GitHub** | Primary | Market leader; strong API; native Actions; team familiarity | App: listens to `pull_request.opened`, `pull_request.synchronize` events; posts review comments via REST API |\n",
    "| **GitLab** | Phase 2 | Support on-prem teams; GitLab CI native | Adapter: MR event → unified PR interface; comment API translated |\n",
    "| **Bitbucket** | Phase 3 | Lower priority; fewer users in org | Adapter pattern |\n",
    "\n",
    "#### CI/CD Orchestration\n",
    "\n",
    "| Tool | Choice | Rationale | Integration |\n",
    "|------|--------|-----------|-------------|\n",
    "| **GitHub Actions** | MVP | Native GitHub integration; free for public repos; easy YAML workflows | Workflow: on `workflow_dispatch` → run tests, build Docker image, push to ECR/GCR, trigger canary deploy |\n",
    "| **Jenkins** | Phase 2 | Support legacy on-prem deployments | Adapter: Jenkins plugin triggers canonical deploy workflow via API |\n",
    "| **GitLab CI** | Phase 2 | GitLab users; native .gitlab-ci.yml | Adapter: GitLab CI pipeline → unified deploy interface |\n",
    "\n",
    "#### Security Scanning (SAST/DAST/SCA)\n",
    "\n",
    "| Tool | Purpose | Choice | Rationale |\n",
    "|---|---|---|---|\n",
    "| **SCA (Dependency Scanning)** | Find vulnerable libraries | Snyk | Fast; free tier; integrates with GitHub; real-time advisories |\n",
    "| **SAST (Code Analysis)** | Find code vulns (CWE-89, -352, etc.) | SonarQube | Deep analysis; Python/Java/JS support; integrates with Actions; on-prem option |\n",
    "| **DAST (Runtime Testing)** | Find vulns in running app | OWASP ZAP | Phase 2 (after MVP deploy); free; can run in canary env |\n",
    "\n",
    "#### Monitoring & Observability\n",
    "\n",
    "| Tool | Purpose | Choice | Rationale |\n",
    "|---|---|---|---|\n",
    "| **Metrics** | Error rate, latency, resource usage | Prometheus | Open-source; easy integration; time-series queries for SLO evaluation |\n",
    "| **Dashboards** | Visualize deploy health; rollback triggers | Grafana | Pairs with Prometheus; real-time alerts; custom dashboards per service |\n",
    "| **Distributed Tracing** | Trace requests across services during deploy | Jaeger | Optional Phase 2; helps correlate deploy impact to customer experience |\n",
    "| **Centralized Logging** | Audit trail of reviews, overrides, rollbacks | ELK Stack (Elasticsearch/Logstash/Kibana) | Or: AWS CloudWatch + Lambda for log analysis |\n",
    "\n",
    "#### Communication & Alerts\n",
    "\n",
    "| Tool | Purpose | Choice | Rationale |\n",
    "|---|---|---|---|\n",
    "| **Slack** | Async notifications (review ready, deploy status, incidents) | Slack Bot | Native integration; low friction; team already uses Slack |\n",
    "| **Jira** | Tracking issues found by AI review | Jira Cloud Integration | Link AI review findings to JIRA tickets; auto-create tickets for critical bugs |\n",
    "| **PagerDuty** | On-call escalation for deploy failures | PagerDuty Webhook | Incident severity → auto-page on-call when rollback triggered |\n",
    "| **Teams** | Org comms (for non-Slack teams) | Phase 2 | Parallel Slack integration |\n",
    "\n",
    "---\n",
    "\n",
    "#### Integration Architecture (simplified)\n",
    "\n",
    "```\n",
    "GitHub (PR Event)\n",
    "  ↓\n",
    "GitHub App\n",
    "  ↓\n",
    "┌─────────────────────────────────────────┐\n",
    "│ Review Engine (Core Service)            │\n",
    "├─────────────────────────────────────────┤\n",
    "│ 1. Fetch diff, repo config              │\n",
    "│ 2. Call AI Reviewer (LLM)               │\n",
    "│ 3. Call SonarQube SAST                  │\n",
    "│ 4. Call Snyk SCA                        │\n",
    "│ 5. Triage & rank findings               │\n",
    "│ 6. Post review comment on GitHub        │\n",
    "│ 7. Emit telemetry → Prometheus          │\n",
    "└─────────────────────────────────────────┘\n",
    "  ↓\n",
    "GitHub Actions (on approval)\n",
    "  ↓\n",
    "┌─────────────────────────────────────────┐\n",
    "│ Deploy Engine                           │\n",
    "├─────────────────────────────────────────┤\n",
    "│ 1. Build Docker image                   │\n",
    "│ 2. Run tests (pytest)                   │\n",
    "│ 3. Deploy canary (5% traffic)           │\n",
    "│ 4. Monitor metrics (Prometheus)         │\n",
    "│ 5. If error_rate > 2%, auto-rollback    │\n",
    "│ 6. Post deploy status → Slack/GitHub    │\n",
    "└─────────────────────────────────────────┘\n",
    "  ↓\n",
    "Observability (Prometheus + Grafana)\n",
    "  ↓\n",
    "Post-Incident Review (Feedback → ML Retraining)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost & Licensing\n",
    "\n",
    "| Tool | Cost | Notes |\n",
    "|------|------|-------|\n",
    "| GitHub App | Free (built-in) | Included in GitHub Enterprise |\n",
    "| GitHub Actions | Free (10K min/month included) | Pay-per-minute after; ~$0.008/min |\n",
    "| SonarQube | $100–500/mo (cloud) | Or self-hosted free tier |\n",
    "| Snyk | Free tier (50 scans/mo); $25–500/mo (enterprise) | Or DIY: Trivy free, lower accuracy |\n",
    "| Prometheus + Grafana | Free (open-source) | Optional managed: Grafana Cloud ~$25–200/mo |\n",
    "| Slack | Included (team subscription) | Or Teams if Slack unavailable |\n",
    "| Jira | Included (team subscription) | Or open-source alternatives (OpenProject) |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**MVP (Months 1–2):** GitHub + Actions + AI review (Python only) + human gate → 2 pilot services\n",
    "**Pilot (Weeks 9–12):** Add Snyk SCA + canary deploy + auto-rollback; validate <4h SLA + <1% FN rate\n",
    "**Rollout (Months 4–5):** Multi-language (Java, Node), broader observability, 30+ services\n",
    "**Hardening (Month 6):** Compliance logging, FP/FN tuning, full adoption\n",
    "\n",
    "**Tool Stack:** GitHub + Actions + SonarQube + Snyk + Prometheus/Grafana + Slack + Jira. All integrate via webhooks/APIs into a central review + deploy engine. Phased rollout de-risks AI adoption; human-in-the-loop until system proven reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
